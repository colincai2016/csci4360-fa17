{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 2: Python Crash Course\n",
    "CSCI 4360/6360: Data Science II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: Python Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python as a language was implemented from the start by Guido van Rossum. What was originally something of a [snarkily-named hobby project to pass the holidays](https://www.python.org/doc/essays/foreword/) turned into a huge open source phenomenon used by millions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![guido](L2/guido.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python's history\n",
    "\n",
    "The original project began in 1989.\n",
    "\n",
    " - Release of Python 2.0 in 2000\n",
    "\n",
    " - Release of Python 3.0 in 2008\n",
    "\n",
    " - Latest stable release of these branches are **2.7.13**--which Guido *emphatically* insists is the final, final, final release of the 2.x branch--and **3.6.2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're welcome to use whatever version you want, just be aware: the AutoLab autograders will be using **3.6.x** (unless otherwise noted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python, the Language\n",
    "\n",
    "Python is an **intepreted** language.\n",
    "\n",
    " - Contrast with **compiled** languages\n",
    " - Performance, ease-of-use\n",
    " - Modern intertwining and blurring of compiled vs interpreted languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Python is a very **general** language.\n",
    "\n",
    " - Not designed as a specialized language for performing a specific task. Instead, it relies on third-party developers to provide these extras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![xkcd](L2/python.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Instead, as [Jake VanderPlas](http://jakevdp.github.io/) put it:\n",
    "\n",
    "> \"Python syntax is the glue that holds your data science code together. As many scientists and statisticians have found, Python excels in that role because it is powerful, intuitive, quick to write, fun to use, and above all extremely useful in day-to-day data science tasks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Language Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic thing possible: Hello, World!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, that's all that's needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Take note: the biggest different between Python 2 and 3 is the `print` function: it technically wasn't a function in Python 2 so much as a *language construct*, and so you didn't need parentheses around the string you wanted printed; in Python 3, it's a full-fledged *function*, and therefore requires parentheses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variables and Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is dynamically-typed, meaning you don't have to declare types when you assign variables. Python is also *duck-typed*, a colloquialism that means it *infers* the best-suited variable type at runtime (\"if it walks like a duck and quacks like a duck...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 5\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 5.5\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It's important to note: even though you don't have to specify a type, Python still assigns a type to variables. It would behoove you to know the types so you don't run into tricky type-related bugs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = 5 * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the type for `x`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y = 5 / 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the type for `y`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are functions you can use to explicitly *cast* a variable from one type to another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 5 / 5\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = int(x)\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = str(y)\n",
    "type(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four main types of built-in Python data structures, each similar but ever-so-slightly different:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Lists (the Python workhorse)\n",
    " 2. Tuples\n",
    " 3. Sets\n",
    " 4. Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(Note: generators and comprehensions are worthy of mention; definitely look into these as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lists are basically your catch-all multi-element data structure; they can hold anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_list = [1, 2, 'something', 6.2, [\"another\", \"list!\"], 7371]\n",
    "print(some_list[3])\n",
    "type(some_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tuples are like lists, except they're *immutable* once you've built them (and denoted by parentheses, instead of brackets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tuple = (1, 2, 'something', 6.2, [\"another\", \"list!\"], 7371)\n",
    "print(some_tuple[5])\n",
    "type(some_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sets are probably the most different: they are mutable (can be changed), but are *unordered* and *can only contain unique items* (they automatically drop duplicates you try to add). They are denoted by braces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'something', 1, 86, 73}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_set = {1, 1, 1, 1, 1, 86, \"something\", 73}\n",
    "some_set.add(1)\n",
    "print(some_set)\n",
    "type(some_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, dictionaries. Other terms that may be more familiar include: maps, hashmaps, or associative arrays. They're a combination of sets (for their *key* mechanism) and lists (for their *value* mechanism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_dict = {\"key\": \"value\", \"another_key\": [1, 3, 4], 3: [\"this\", \"value\"]}\n",
    "print(some_dict[\"another_key\"])\n",
    "type(some_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries explicitly set up a mapping between a *key*--keys are unique and unordered, exactly like sets--to *values*, which are an arbitrary list of items. These are very powerful structures for data science-y applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Slicing and Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordered data structures in Python are 0-indexed (like C, C++, and Java). This means the first elements are at index 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 'something', 6.2, ['another', 'list!'], 7371]\n"
     ]
    }
   ],
   "source": [
    "print(some_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(some_list[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, using colon notation, you can \"slice out\" entire sections of ordered structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 'something']\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "end = 3\n",
    "print(some_list[start : end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the starting index is *inclusive*, but the ending index is *exclusive*. Also, if you omit the starting index, Python assumes you mean 0 (start at the beginning); likewise, if you omit the ending index, Python assumes you mean \"go to the very end\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 'something']\n"
     ]
    }
   ],
   "source": [
    "print(some_list[:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 'something', 6.2, ['another', 'list!'], 7371]\n"
     ]
    }
   ],
   "source": [
    "start = 1\n",
    "print(some_list[start:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loops\n",
    "\n",
    "Python supports two kinds of loops: `for` and `while`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`for` loops in Python are, in practice, closer to *for each* loops in other languages: they iterate through collections of items, rather than incrementing indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "something\n",
      "6.2\n",
      "['another', 'list!']\n",
      "7371\n"
     ]
    }
   ],
   "source": [
    "for item in some_list:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - the collection to be iterated through is at the end (`some_list`)\n",
    " - the current item being iterated over is given a variable after the `for` statement (`item`)\n",
    " - the loop body says what to do in an iteration (`print(item)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But if you need to iterate by index, check out the `enumerate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1\n",
      "1: 2\n",
      "2: something\n",
      "3: 6.2\n",
      "4: ['another', 'list!']\n",
      "5: 7371\n"
     ]
    }
   ],
   "source": [
    "for index, item in enumerate(some_list):\n",
    "    print(\"{}: {}\".format(index, item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`while` loops operate as you've probably come to expect: there is some associated boolean condition, and as long as that condition remains `True`, the loop will keep happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < 10:\n",
    "    print(i)\n",
    "    i += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**IMPORTANT**: Do not forget to perform the *update* step in the body of the `while` loop! After using `for` loops, it's easy to become complacent and think that Python will update things automatically for you. If you forget that critical `i += 2` line in the loop body, this loop will go on forever..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another cool looping utility when you have multiple collections of identical length you want to loop through simultaneously: the `zip()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 7\n",
      "2 5 8\n",
      "3 6 9\n"
     ]
    }
   ],
   "source": [
    "list1 = [1, 2, 3]\n",
    "list2 = [4, 5, 6]\n",
    "list3 = [7, 8, 9]\n",
    "\n",
    "for x, y, z in zip(list1, list2, list3):\n",
    "    print(\"{} {} {}\".format(x, y, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"zips\" together the lists and picks corresponding elements from each for every loop iteration. Way easier than trying to set up a numerical index to loop through all three simultaneously, but you can even combine this with `enumerate` to do exactly that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: (1, 4, 7)\n",
      "1: (2, 5, 8)\n",
      "2: (3, 6, 9)\n"
     ]
    }
   ],
   "source": [
    "for index, (x, y, z) in enumerate(zip(list1, list2, list3)):\n",
    "    print(\"{}: ({}, {}, {})\".format(index, x, y, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditionals\n",
    "\n",
    "Conditionals, or `if` statements, allow you to branch the execution of your code depending on certain circumstances.\n",
    "\n",
    "In Python, this entails three keywords: `if`, `elif`, and `else`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    }
   ],
   "source": [
    "grade = 82\n",
    "if grade > 90:\n",
    "    print(\"A\")\n",
    "elif grade > 80:\n",
    "    print(\"B\")\n",
    "else:\n",
    "    print(\"Something else\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple important differences from C/C++/Java parlance:\n",
    " - **NO** parentheses around the boolean condition!\n",
    " - It's not \"`else if`\" or \"`elseif`\", just \"`elif`\". It's admittedly weird, but it's Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Conditionals, when used with loops, offer a powerful way of slightly tweaking loop behavior with two keywords: `continue` and `break`.\n",
    "\n",
    "The former is used when you want to skip an iteration of the loop, but nonetheless keep going on to the *next* iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4\n",
      "1.2\n",
      "6898.32\n",
      "5289.24\n",
      "25.1\n",
      "52.4\n"
     ]
    }
   ],
   "source": [
    "list_of_data = [4.4, 1.2, 6898.32, \"bad data!\", 5289.24, 25.1, \"other bad data!\", 52.4]\n",
    "\n",
    "for x in list_of_data:\n",
    "    if type(x) == str:\n",
    "        continue\n",
    "    \n",
    "    # This stuff gets skipped anytime the \"continue\" is run\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`break`, on the other hand, literally slams the brakes on a loop, pulling you out one level of indentation immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "i = 0\n",
    "iters = 0\n",
    "while True:\n",
    "    iters += 1\n",
    "    i += random.randint(0, 10)\n",
    "    if i > 1000:\n",
    "        break\n",
    "\n",
    "print(iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### File I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has a great file I/O library. There are usually third-party libraries that expedite reading certain often-used formats (JSON, XML, binary formats, etc), but you should still be familiar with input/output handles and how they work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text_to_write = \"I want to save this to a file.\"\n",
    "f = open(\"some_file.txt\", \"w\")\n",
    "f.write(text_to_write)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code writes the string on the first line to a file named `some_file.txt`. We can read it back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to save this to a file.\n"
     ]
    }
   ],
   "source": [
    "f = open(\"some_file.txt\", \"r\")\n",
    "from_file = f.read()\n",
    "f.close()\n",
    "print(from_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note what changed: when writing, we used a `\"w\"` character in the `open` argument, but when reading we used `\"r\"`. Hopefully this is easy to remember.\n",
    "\n",
    "Also, when reading/writing *binary* files, you have to include a \"b\": `\"rb\"` or `\"wb\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A core tenet in writing functions is that **functions should do one thing, and do it well**.\n",
    "\n",
    "Writing good functions makes code *much* easier to troubleshoot and debug, as the code is already logically separated into components that perform very specific tasks. Thus, if your application is breaking, you usually have a good idea where to start looking.\n",
    "\n",
    "**WARNING**: It's very easy to get caught up writing \"god functions\": one or two massive functions that essentially do everything you need your program to do. But if something breaks, this design is very difficult to debug.\n",
    "\n",
    "Homework assignments will often require you to break your code into functions so different portions can be autograded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Functions have a header definition and a body:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def some_function():  # This line is the header\n",
    "    pass              # Everything after (that's indented) is the body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function doesn't do anything, but it's perfectly valid. We can call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "some_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Not terribly interesting, but a good outline. To make it interesting, we should add input arguments and return values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def vector_magnitude(vector):\n",
    "    d = 0.0\n",
    "    for x in vector:\n",
    "        d += x ** 2\n",
    "    return d ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "v1 = [1, 1]\n",
    "d1 = vector_magnitude(v1)\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.95862079783298\n"
     ]
    }
   ],
   "source": [
    "v2 = [53.3, 13.4]\n",
    "d2 = vector_magnitude(v2)\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NumPy Arrays\n",
    "\n",
    "If you looked at our previous `vector_magnitude` function and thought \"there must be an easier way to do this\", then you were correct: that easier way is NumPy arrays.\n",
    "\n",
    "NumPy arrays are the result of taking Python lists and adding a ton of back-end C++ code to make them *really* efficient.\n",
    "\n",
    "Two areas where they excel: *vectorized programming* and *fancy indexing*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vectorized programming is perfectly demonstrated with our previous `vector_magnitude` function: since we're performing the same operation on every element of the vector, NumPy allows us to build code that implicitly handles the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorized_magnitude(vector):\n",
    "    return (vector ** 2).sum() ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.41421356237\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([1, 1])\n",
    "d1 = vectorized_magnitude(v1)\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.9586207978\n"
     ]
    }
   ],
   "source": [
    "v2 = np.array([53.3, 13.4])\n",
    "d2 = vectorized_magnitude(v2)\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We've also seen indexing and slicing before; here, however, NumPy really shines.\n",
    "\n",
    "Let's say we have some super high-dimensional data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.random((500, 600, 250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can take statistics of any dimension or slice we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4999548512446948"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:400, 100:200, 0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0028883487428047415"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[X < 0.01].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.51867524,  0.51729758,  0.46728925,  0.51712595,  0.48342916,\n",
       "        0.49878547,  0.55277146,  0.481717  ,  0.47940307,  0.50020324,\n",
       "        0.49276312,  0.47972106,  0.46755515,  0.47643823,  0.52474592,\n",
       "        0.49044803,  0.47181944,  0.4414494 ,  0.49613891,  0.5069655 ,\n",
       "        0.53519317,  0.48736629,  0.48582798,  0.48059914,  0.49855154,\n",
       "        0.50373656,  0.48236145,  0.54419453,  0.53897251,  0.43914593,\n",
       "        0.51173516,  0.52868761,  0.51176298,  0.51731556,  0.52547951,\n",
       "        0.53169123,  0.52894985,  0.46413039,  0.46841093,  0.48413696,\n",
       "        0.51681629,  0.4906845 ,  0.47991217,  0.48928538,  0.51898214,\n",
       "        0.46979085,  0.51911555,  0.49553128,  0.5362568 ,  0.48612895,\n",
       "        0.47753441,  0.50153655,  0.52788072,  0.52272401,  0.469157  ,\n",
       "        0.49238255,  0.47559093,  0.49801046,  0.52056792,  0.51585418,\n",
       "        0.469604  ,  0.49038037,  0.55188759,  0.50849742,  0.51667988,\n",
       "        0.5342947 ,  0.50331315,  0.44969043,  0.51828591,  0.43520427,\n",
       "        0.48280764,  0.47801979,  0.55527671,  0.54282912,  0.47998791,\n",
       "        0.53462367,  0.51330914,  0.46526063,  0.48525525,  0.55084526,\n",
       "        0.53366459,  0.46832472,  0.5182397 ,  0.46299839,  0.50300485,\n",
       "        0.52058732,  0.47696937,  0.43319653,  0.53635848,  0.52606759,\n",
       "        0.44742076,  0.53064638,  0.45443005,  0.48121565,  0.489155  ,\n",
       "        0.46740624,  0.52551917,  0.49859357,  0.50319623,  0.49630816,\n",
       "        0.48928332,  0.49311857,  0.48556844,  0.4731037 ,  0.46989062,\n",
       "        0.47058212,  0.5219647 ,  0.52420511,  0.48058429,  0.48308218,\n",
       "        0.48383466,  0.47922008,  0.45676722,  0.479294  ,  0.48776054,\n",
       "        0.51039874,  0.48538941,  0.482676  ,  0.44476912,  0.48895528,\n",
       "        0.50207127,  0.53486804,  0.49224779,  0.5057927 ,  0.50089204,\n",
       "        0.49259557,  0.44914932,  0.47931206,  0.47980468,  0.48720734,\n",
       "        0.49625651,  0.519241  ,  0.48548304,  0.50987037,  0.48421921,\n",
       "        0.48875296,  0.54722896,  0.51664063,  0.47688314,  0.4758808 ,\n",
       "        0.50993576,  0.56979423,  0.53983663,  0.48248539,  0.47952436,\n",
       "        0.46437517,  0.54340922,  0.45194619,  0.45818344,  0.52343475,\n",
       "        0.57352936,  0.53265172,  0.44577159,  0.49278033,  0.52293339,\n",
       "        0.53243724,  0.49470642,  0.51400523,  0.49752551,  0.50414942,\n",
       "        0.52920021,  0.54674973,  0.52547939,  0.50266327,  0.50805356,\n",
       "        0.48169875,  0.55863358,  0.46440341,  0.51636321,  0.45606104,\n",
       "        0.48065807,  0.54691791,  0.51672346,  0.49402188,  0.53044119,\n",
       "        0.48142973,  0.50331395,  0.50838829,  0.53598983,  0.49939437,\n",
       "        0.50970022,  0.50900828,  0.47224788,  0.48400372,  0.54702908,\n",
       "        0.6038528 ,  0.54586053,  0.45575719,  0.47799725,  0.51258097,\n",
       "        0.45549159,  0.54565552,  0.52947205,  0.54935615,  0.51325031,\n",
       "        0.49821494,  0.49455383,  0.52963757,  0.51744372,  0.5432769 ,\n",
       "        0.50843641,  0.49398491,  0.50603989,  0.49051012,  0.46860556,\n",
       "        0.54842711,  0.49851335,  0.50006079,  0.5288276 ,  0.4565634 ,\n",
       "        0.50009507,  0.49696259,  0.55482749,  0.50158645,  0.53907227,\n",
       "        0.55335015,  0.50565859,  0.49504051,  0.47154726,  0.49958723,\n",
       "        0.50459533,  0.4601803 ,  0.52646739,  0.48584911,  0.50740882,\n",
       "        0.52598056,  0.48098476,  0.55930797,  0.49809867,  0.45366715,\n",
       "        0.50646057,  0.46895451,  0.53895226,  0.49611525,  0.51050281,\n",
       "        0.52999852,  0.52002761,  0.46488792,  0.54325472,  0.47775997,\n",
       "        0.55790064,  0.43938541,  0.51835798,  0.50141219,  0.50459811,\n",
       "        0.48380416,  0.53934964,  0.48985159,  0.51079463,  0.47167189,\n",
       "        0.45118765,  0.51404674,  0.50198744,  0.47907865,  0.50424693,\n",
       "        0.50845228,  0.51585481,  0.47079571,  0.44718341,  0.5015922 ,\n",
       "        0.50932904,  0.49724879,  0.44532206,  0.52902114,  0.53207428,\n",
       "        0.45166003,  0.52560117,  0.52039181,  0.49152202,  0.5351256 ,\n",
       "        0.52300715,  0.51161438,  0.500493  ,  0.50991224,  0.49017083,\n",
       "        0.53667429,  0.54666195,  0.50169257,  0.48367601,  0.50505634,\n",
       "        0.51529964,  0.50867079,  0.5012247 ,  0.51413904,  0.48003921,\n",
       "        0.48788705,  0.52335261,  0.48861606,  0.50726088,  0.48286214,\n",
       "        0.49508184,  0.50207396,  0.50320645,  0.52371617,  0.51244911,\n",
       "        0.51063896,  0.451227  ,  0.46406436,  0.51526914,  0.46889669,\n",
       "        0.49210771,  0.50886323,  0.47401079,  0.49884928,  0.47263763,\n",
       "        0.49168413,  0.49949037,  0.52709994,  0.50780375,  0.53584516,\n",
       "        0.51078143,  0.52435315,  0.51844322,  0.51138457,  0.49690471,\n",
       "        0.50502691,  0.49701969,  0.46849427,  0.52716325,  0.49590413,\n",
       "        0.52929819,  0.49757602,  0.49687554,  0.49016215,  0.53120804,\n",
       "        0.50878902,  0.46620647,  0.47068681,  0.48963287,  0.50216615,\n",
       "        0.47197334,  0.47046874,  0.53400009,  0.48733264,  0.45192248,\n",
       "        0.47128717,  0.47485001,  0.51000274,  0.52965288,  0.49683928,\n",
       "        0.45040037,  0.44120822,  0.45106112,  0.52628488,  0.51305737,\n",
       "        0.52952149,  0.53244068,  0.51292314,  0.48635673,  0.46946239,\n",
       "        0.46254022,  0.52060135,  0.49953953,  0.48472247,  0.52934475,\n",
       "        0.50739332,  0.51218768,  0.48863851,  0.48882609,  0.45735577,\n",
       "        0.54044169,  0.46743286,  0.49637654,  0.44815554,  0.4792592 ,\n",
       "        0.46523942,  0.50832695,  0.47146546,  0.5224416 ,  0.46677961,\n",
       "        0.46138582,  0.55838296,  0.48326638,  0.50875215,  0.50351185,\n",
       "        0.53086144,  0.50254595,  0.51719558,  0.50168972,  0.56349876,\n",
       "        0.48710207,  0.51634076,  0.52657891,  0.4993885 ,  0.47442877,\n",
       "        0.47739874,  0.51438403,  0.50740797,  0.52884893,  0.46762566,\n",
       "        0.48455616,  0.49551488,  0.47547752,  0.48632515,  0.49914437,\n",
       "        0.5058193 ,  0.49056452,  0.49961336,  0.45312081,  0.49036715])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:400, 100:200, 0].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3: Document Classification with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll end our Python crash-course with a bit of a review from 3360 or your previous intro-to-ML experience: document classification with **Naive Bayes** and **Logistic Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you're familiar with this abstraction for modeling documents.\n",
    "\n",
    "This model assumes that each word in a document is drawn independently from a multinomial distribution over possible words (a multinomial distribution is a generalization of a Bernoulli distribution to multiple values). Although this model ignores the ordering of words in a document, it works surprisingly well for a number of tasks, including classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, it says: word *order* doesn't matter nearly as much--or perhaps, at all--as word *frequency*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![bow](L2/bag-of-words.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes\n",
    "\n",
    "With any (discriminative) classification problem, you're asking: what's the probability of a label given the data? In our document classification example, this question is: what is the probability of the document class, given the document itself?\n",
    "\n",
    "Formally, for a document $x$ and label $y$: $P(y | x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we're using individual word counts as features ($x_1$ is word 1, $x_2$ is word 2, and so on), then by the rules of conditional probability, this probability would expand into something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(y | x_1, x_2, ..., x_n) = \\frac{P(y)P(x_1, x_2, ..., x_n)}{P(x_1, x_2, ..., x_n)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is, for all practical purposes, intractable. Hence, \"naive\": we make each word conditionally independent of the others, *given* the label:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(x_i | y, x_1, x_2, ..., x_i, x_{i + 1}, ..., x_n) = P(x_i | y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For any given word $x_i$ then, the original problem reduces to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(y | x_1, x_2, ..., x_n) = \\frac{P(y) \\Pi_{i = 1}^n P(x_i | y)}{P(x_1, x_2, ..., x_n)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And since the denominator is the same across all documents, we can effectively ignore it as a constant, thereby giving us a decision:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} = \\textrm{argmax}_y P(y) \\Pi_{i = 1}^n P(x_i | y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you really want to dig into what makes Naive Bayes an improvement over the \"optimal Bayes classifier\", you can count exactly how many *parameters* are required in either case.\n",
    "\n",
    "We'll take the simple example: the decision variable $Y$ is boolean, and the observations $X$ have $n$ attributes, each of which is also boolean. Formally, that looks like this:\n",
    "\n",
    "$$\n",
    "\\theta_{ij} = P(X = x_i | Y = y_i)\n",
    "$$\n",
    "\n",
    "where $i$ takes on $2^n$ possible values (one for each of the possible combinations of boolean values in the array $X$, and $j$ takes on 2 possible values (true or false). For any fixed $j$ the sum over $i$ of $\\theta_{ij}$ has to be 1 (probability). So for any particular $y_j$, you have the $2^{n}$ values of $x_i$, so you need $2^n - 1$ parameters. Given two possible values for $j$ (since $Y$ is boolean!), we must estimate a total of $2 (2^n - 1)$ such $\\theta_{ij}$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**This is a problem!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This means that, if our observations $X$ have three attributes--3-dimensional data--we need 14 distinct data points *at least*, one for each possible boolean combination of attributes in $X$ and label $Y$. It gets exponentially worse as the number of boolean attributes increases--if $X$ has 30 boolean attributes, we'll have to estimate **30 billion parameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is why the conditional independence assumption of Naive Bayes is so critical: more than anything, it **substantially** reduces the number of required estimated parameters. If, through conditional independence, we have\n",
    "\n",
    "$$\n",
    "P(X_1, X_2, ..., X_n | Y) = \\Pi_{i = 1}^n P(X_i | Y)\n",
    "$$\n",
    "\n",
    "or, to illustrate more concretely, observations $X$ with 3 attributes each\n",
    "\n",
    "$$\n",
    "P(X_1, X_2, X_3 | Y) = P(X_1 | Y) P(X_2 | Y) P(X_3 | Y)\n",
    "$$\n",
    "\n",
    "we've just gone from requiring the aforementioned 14 parameters, to 6!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Formally: we've gone from requiring $2(2^n - 1)$ parameters to $2n$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Naive Bayes is a fantastic algorithm and works well in practice. However, it has some important drawbacks to be aware of:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - **Data may not be conditionally independent.** The easiest example of this: replicating a single observation multiple times. These are *clearly* dependent entities, but Naive Bayes will treat them as independent of each other, given the class label. In practice this isn't a common occurrence but can happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - **What about continuous attributes?** We've only looked so far at data with boolean attributes; most data, including documents, are not boolean. Rather, they are *continuous*. We can fairly easily modify Naive Bayes to *Gaussian Naive Bayes,* where each attribute is an i.i.d. Gaussian, but this introduces new problems: now we're assuming our data are Gaussian, which like conditional independence, may not be true in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - **Observing data in *testing* that was not observed in *training*.** With the document classification example, training the Naive Bayes parameters essentially consists of word counting. However, what happens when you encounter a word $X_i$ in a test set for which you do not have a corresponding $P(X_i | Y)$? By default, this sets a probability of 0, but this is problematic in a Naive Bayes setting: since you're computing $P(X_1 | Y) P(X_2 | Y) ... P(X_n | Y)$ for a document, a single probability of 0 in that string of multiplication nukes the entire statement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic regression is a bit different. Rather than estimating the parametric form of the data $P(x_i | y)$ and $P(y)$ in order to get to the posterior $P(y | x)$, here we're learning the decision boundary $P(y | x)$ *directly*.\n",
    "\n",
    "Ideally we want some kind of output function between 0 and 1--so let's just go with the *logit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x135ccadd8>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl0ldW9//H3l8wjQ4gQSEKYB0EFwqDcOtUBccDr9aeo\nUGuttN7a28E6tLa2td6frb3WamtVrEOpIw612GLRKq3WChLmMRDCkBAgCSEhc3KSff9I9KYYzAFO\n8pzh81qLRZ5znpV8Dkk+a7PP8+xtzjlERCS89PI6gIiIBJ7KXUQkDKncRUTCkMpdRCQMqdxFRMKQ\nyl1EJAyp3EVEwpDKXUQkDKncRUTCULRXX7h///4uJyfHqy8vIhKSVq1aVe6cS+/qPM/KPScnh7y8\nPK++vIhISDKz3f6cp2kZEZEwpHIXEQlDKncRkTDUZbmb2VNmVmpmG4/yvJnZw2ZWYGbrzWxS4GOK\niMix8Gfk/gww8zOevwgY2f5nPvDoiccSEZET0WW5O+feAyo+45TZwELXZjnQx8wyAhVQRESOXSDm\n3AcDRR2Oi9sfExERjwTiOnfr5LFO9+4zs/m0Td2QnZ0dgC8tIhIcnHPUN7dQ3eCjusFHTaOP2sb/\n+7u2qYW6Rh91TS2cO+YkTs3q0615AlHuxUBWh+NMoKSzE51zC4AFALm5udq8VUSCUkur42BtI+XV\nTVTUNnGwtpFDtU1U1DVzqLaJyvpmKuuaqKpvpqq+meoGH4frm/G1+ldr6SlxIVHui4FbzOxFYBpQ\n5ZzbF4DPKyIScLWNPvZW1lNSWc++qgb2VTVwoKqBA9UNHDjcSFl1AxW1TXTW02bQOyGGPgkx9EmM\npW9iLDlpSfROiCE1IZqU+BhS4qNJjosmJT6apNhokuLajpPiokmKiyI+OopevTqb8AisLsvdzF4A\nzgb6m1kx8EMgBsA59xiwBJgFFAB1wA3dFVZEpCvOOSpqm9hZXktheS27D9ay+2AdeyrqKKqo41Bd\n87+cbwb9k+MYmBrP4D7xnJbVm/TkONJT4khLjiMtKZa05Dj6JcXSOyGGqB4o5kDostydc9d08bwD\nvhawRCIifqqobWLrvsNs2V9NQWk12w/UsL20hqr6/yvw6F7G4L4JZPdLZPyEDAb3SSCzbwKD+iSQ\n0TueAanxxESF3/2cni0cJiJyLMprGllXVMn64io27q1iY0kVBw43fvJ8v6RYRp6UzCWnZDA8PZmh\n6UkMTUsis28C0WFY3l1RuYtI0GltdWwvreGjXRXk7apgzZ5K9lTUAW3TKCPSkzljeH/GZaQyNiOV\n0QNTSE+J8zh1cFG5i4jnnHPsLK/lgx0H+WB7Oct3HqSyfW58QGock7L7Mnd6Nqdl9eXkQakkxam6\nuqJ/IRHxRENzCx/uOMiy/FKW5ZdSVFEPwOA+CZw3dgDThvZj2tA0svolYBYab2IGE5W7iPSYww3N\nvLullKWb9vO3/DLqm1tIiIlixog05p85nM+N6M+QtESVeQCo3EWkWzU0t/DOllIWr9vLsvwymnyt\npKfEccWkwVxw8kCmDe1HfEyU1zHDjspdRALOOceaokpezivmT+tKqG70kZ4Sx7VTs7n01AwmZvXt\nkRt5IpnKXUQCprqhmT+s2cuzy3ez7UAN8TG9mDUhg/+YlMn0YWkhcwNQOFC5i8gJ21ley1P/2Mmr\nq4upa2rhlMze3HfFBC45JYOU+Biv40UklbuIHLdVuyt4/O+FvL3lADG9enHZaYOYN31Ity+KJV1T\nuYvIMXHOsbywgoff2c6HhQfpkxjD188ZwbzTc3QjURBRuYuI3/J2VXD/0nw+2llBekocP7hkHNdM\nzSIxVlUSbPQdEZEu5e+v5udLt/LXLaWkp8Txo0vHMWdqti5hDGIqdxE5qoraJh54K58XPtpDUmw0\nt104mhtm5GikHgL0HRKRT2lpdTy7fDe/eHsbNY0+vnB6Dt/4/Ej6JsV6HU38pHIXkX+xcW8V331t\nAxv2VvFvI/pz96XjGDUgxetYcoxU7iICQH1TCw+8lc9TH+wkLTmOR66dxKwJA7XOS4hSuYsIq3ZX\n8J2X17OzvJZrp2Vzx8wx9E7QzUehTOUuEsEafS08+PZ2Fry3g4zeCbxw03ROH57mdSwJAJW7SITa\nWV7L119Yzca9h5kzJYvvXzKOZG2CETb0nRSJQK+v2ctdf9hAdFQvFsybzAUnD/Q6kgSYyl0kgjT6\nWvjR4s288NEepuT05aE5ExnUJ8HrWNINVO4iEWJfVT1ffXY164oqufns4dx6/iiio3p5HUu6icpd\nJAKs3FXBzc+uor6phcfmTmLm+AyvI0k3U7mLhLnXVhdz56sbGNy37WqYkbohKSKo3EXCVGur48G/\nbuNX7xZw+rA0Hp07iT6JWj4gUqjcRcJQk6+V219Zx+trS7gqN5N7L59AbLTm1yOJyl0kzNQ2+rj5\nudW8t62M71wwiq+dM0JLCEQglbtIGKmobeKGZ1ayobiSn14xgTlTs72OJB5RuYuEidLqBq57YgV7\nKup4bK5uTIp0KneRMLC/qoFrn1jOvqoGnr5hCmcM7+91JPGYX++wmNlMM8s3swIzu7OT57PNbJmZ\nrTGz9WY2K/BRRaQzeyvruXrBh5RWN7LwxqkqdgH8KHcziwIeAS4CxgHXmNm4I077PrDIOTcRmAP8\nJtBBReTT9lXVc82C5VTUNvH7G6cyJaef15EkSPgzcp8KFDjnCp1zTcCLwOwjznFAavvHvYGSwEUU\nkc58PMfeVuzTmJjd1+tIEkT8mXMfDBR1OC4Gph1xzo+At8zs60AScF5A0olIpypqm5j72xXsq2pg\n4Y1TOS2rj9eRJMj4M3Lv7AJZd8TxNcAzzrlMYBbwezP71Oc2s/lmlmdmeWVlZceeVkSobmjmC0+t\nYPfBOp68PldTMdIpf8q9GMjqcJzJp6ddbgQWATjnPgTigU+9q+OcW+Ccy3XO5aanpx9fYpEI1uhr\n4avPrmLLvmoenTuJM0bozVPpnD/lvhIYaWZDzSyWtjdMFx9xzh7g8wBmNpa2ctfQXCSAWlod3160\njg8KDvLzK0/h3DEDvI4kQazLcnfO+YBbgKXAFtquitlkZveY2WXtp90K3GRm64AXgC86546cuhGR\n4+Sc4543NvHn9fu4a9ZYrpiU6XUkCXJ+3cTknFsCLDnisbs7fLwZmBHYaCLysSf/sZPffbibmz43\nlJvOHOZ1HAkBWiZOJMj9ZeN+/nvJFi4aP5DvXjTW6zgSIlTuIkFsXVEl33xpDadm9uHBq0+jVy+t\n7ij+UbmLBKl9VfV8eWEe/ZPjeOILucTHRHkdSUKIyl0kCDU0t/CV36+irtHHk9dPIT0lzutIEmK0\nKqRIkHHOceer61lfXMWCeZMZPVB7nsqx08hdJMgseK+Q19eWcOv5o7Qmuxw3lbtIEPmgoJyf/WUr\nF0/I4JZzR3gdR0KYyl0kSJRU1vP1F9YwPD2Z+688RfueyglRuYsEgSZfK//53Goam1t4dO5kkuL0\ndpicGP0EiQSBe/+8mbVFlfzmukmMOCnZ6zgSBjRyF/HYn9aXsLB9aYFZEzK8jiNhQuUu4qHdB2u5\n89UNTMzuw+0zx3gdR8KIyl3EI42+Fm55fg29DH51zURiovTrKIGjOXcRj/z0za1s2FvF4/Mmk9k3\n0es4EmY0VBDxwDtbDvD0B7v44hk5XKgblaQbqNxFelhpdQO3v7KeMQNT+O4szbNL99C0jEgPcs5x\n28vrqWn08cL86cRFa6VH6R4auYv0oGf+uYu/byvjrovHMmqAFgST7qNyF+kh2w5Uc9+bWzl3zEnM\nmz7E6zgS5lTuIj2gydfKt15aS0pctNaNkR6hOXeRHvCrd7ezqeQwC+ZNpn+yNt6Q7qeRu0g3W73n\nEI8sK+DKyZlan116jMpdpBvVN7Vw66J1ZPRO4O5Lx3kdRyKIpmVEutH9S7eys7yW52+aRmp8jNdx\nJIJo5C7STT7aWcEz/9zF9acP4Yzh/b2OIxFG5S7SDeqbWrj9lXVk9k3Qao/iCU3LiHSD/3krn10H\n63j+pmnaVUk8oZG7SIDl7argqQ92Mm+6pmPEOyp3kQBqaG7hjlfXM6h3AndcpOkY8Y7+vygSQL9+\nt4AdZbUs/NJUkjUdIx7SyF0kQDaXHOaxv+/gPyZlcuaodK/jSITzq9zNbKaZ5ZtZgZndeZRzrjKz\nzWa2ycyeD2xMkeDma2nljlfX0ycxhh9cMtbrOCJdT8uYWRTwCHA+UAysNLPFzrnNHc4ZCXwXmOGc\nO2RmJ3VXYJFg9NQHO9mwt4rfXDeJPomxXscR8WvkPhUocM4VOueagBeB2UeccxPwiHPuEIBzrjSw\nMUWCV1FFHb94exvnjR3AReO1dowEB3/KfTBQ1OG4uP2xjkYBo8zsAzNbbmYzAxVQJJg557jr9Y1E\nmfGTy0/WUr4SNPx5O7+zn1bXyecZCZwNZALvm9l451zlv3wis/nAfIDs7OxjDisSbBavK+G9bWX8\n+LKTyeid4HUckU/4M3IvBrI6HGcCJZ2c80fnXLNzbieQT1vZ/wvn3ALnXK5zLjc9XVcTSGg7VNvE\nPW9s5rSsPszVzkoSZPwp95XASDMbamaxwBxg8RHnvA6cA2Bm/WmbpikMZFCRYHPfm1uoqm/mvism\nENVL0zESXLosd+ecD7gFWApsARY55zaZ2T1mdln7aUuBg2a2GVgG3OacO9hdoUW8tqLwIIvyivny\n54YxNiPV6zgin2LOHTl93jNyc3NdXl6eJ19b5EQ0+VqZ9fD7NDS38Pa3ziIhNsrrSBJBzGyVcy63\nq/N0f7TIMVrw3g4KSmt4+oYpKnYJWlp+QOQY7Cqv5VfvFnDxhAzOGa179SR4qdxF/OSc4wd/3EhM\nVC/thypBT+Uu4qc/b9jH+9vL+c4FoxiQGu91HJHPpHIX8UN1QzP3vLGZ8YNTmXd6jtdxRLqkN1RF\n/PDAW9soq2nkiS/k6pp2CQkauYt0YePeKhZ+uIvrpmVzalYfr+OI+EXlLvIZWlrbFgbrlxTLbRdq\n2zwJHSp3kc/w4so9rCuq5K6Lx9I7IcbrOCJ+U7mLHEV5TSP3/yWfaUP7cflpR65yLRLcVO4iR/HT\nN7dS2+jj3svHa512CTkqd5FOfLSzgldWFXPTmcMYOSDF6zgix0zlLnKE5pZWfvD6Rgb3SeDr547w\nOo7IcVG5ixzhd//cRf6Bau6+dByJsboVREKTyl2kg/1VDTz49jbOHXMSF4wb4HUckeOmchfp4Cd/\n3oyv1fGjS7XZtYQ2lbtIu/e2lfHn9fv42jkjyE5L9DqOyAlRuYsAjb4Wfrh4Ezlpicw/c5jXcURO\nmN4tEgEW/L2QneW1LPzSVOJjtLuShD6N3CXi7TlYx6+Xte2udOaodK/jiASEyl0imnOOHy7eSHQv\n4weXaHclCR8qd4loSzcdYFl+Gd86fxQDe2t3JQkfKneJWLWNPu55YxNjBqZw/Rk5XscRCSiVu0Ss\nh9/ZTklVA/dePp6YKP0qSHjRT7REpPz91Tz5j51cnZtFbk4/r+OIBJzKXSJOa6vj+69vICU+mjsv\n0u5KEp5U7hJxXlldzMpdh/jurLH0TYr1Oo5It1C5S0SpqG3iviVbmJLTlysnZXodR6TbqNwloty3\nZAvVDT7uvXwCvXppYTAJXyp3iRjLCw/ycvvuSqMHanclCW8qd4kIjb4W7vrDBrL6JfBf5470Oo5I\nt/Or3M1sppnlm1mBmd35GeddaWbOzHIDF1HkxC34eyE7ymq5Z/Z4EmK1MJiEvy7L3cyigEeAi4Bx\nwDVm9qlFOMwsBfgvYEWgQ4qciJ3ltfyqfWGwc0af5HUckR7hz8h9KlDgnCt0zjUBLwKzOznvJ8D9\nQEMA84mcEOcc33ttA3HRvbj7Ui0MJpHDn3IfDBR1OC5uf+wTZjYRyHLO/SmA2URO2Curivmw8CB3\nXjSGAalaGEwihz/l3tn1Yu6TJ816AQ8Ct3b5iczmm1memeWVlZX5n1LkOJTXNPLfS7aQO6Qv10zJ\n9jqOSI/yp9yLgawOx5lASYfjFGA88Dcz2wVMBxZ39qaqc26Bcy7XOZebnq5NEaR73funzdQ2+rjv\nCl3TLpHHn3JfCYw0s6FmFgvMARZ//KRzrso51985l+OcywGWA5c55/K6JbGIH5bll/L62hJuPms4\nIwfomnaJPF2Wu3POB9wCLAW2AIucc5vM7B4zu6y7A4ocq5pGH3e9toERJyXztXNHeB1HxBN+bZDt\nnFsCLDnisbuPcu7ZJx5L5Pj9/C9b2Xe4gVe+ejpx0bqmXSKT7lCVsLJyVwULl+/m+tNzmDxE67RL\n5FK5S9hoaG7hjlfXM6h3ArddONrrOCKe8mtaRiQU/PKv2yksq2Xhl6aSFKcfbYlsGrlLWFhbVMmC\n93ZwdW4WZ47SZbYiKncJeQ3NLdz28joGpMZz1yVjvY4jEhT0f1cJeQ+/s53tpTU8c8MUUuNjvI4j\nEhQ0cpeQtraoksffK+Sq3EzO1oqPIp9QuUvIqm9q4duL1jIgJY67LtaKjyIdaVpGQtbP/rKVwrJa\nnvvyNHonaDpGpCON3CUkfVBQzjP/3MUXz8hhxoj+XscRCToqdwk5VfXN3PbyOoalJ3HHzDFexxEJ\nSpqWkZBz9x83cqC6kVdvPkP7oYochUbuElJeX7OXP64t4RufH8lpWX28jiMStFTuEjKKKur4/usb\nmZLTl6+do6V8RT6Lyl1Cgq+llW++tBYDfnHVaURpZyWRz6Q5dwkJD79bwKrdh3hozmlk9Uv0Oo5I\n0NPIXYLePwvK+dW727li0mBmnzbY6zgiIUHlLkGtvKaRb7y0lmH9k/jJ7PFexxEJGZqWkaDV2ur4\n1ktrqapv1hrtIsdII3cJWo/+fQfvby/nh5eOY2xGqtdxREKKyl2C0j+2l/PAW/lceuogrp2a7XUc\nkZCjcpegU1JZz3+9uIbh6cn89IoJmOmyR5FjpXKXoNLoa+Hm51bT5GvlsXmTNc8ucpz0myNB5cdv\nbGZdUSWPXjeJ4enJXscRCVkauUvQ+P3y3Ty/Yg9fOWsYF03I8DqOSEhTuUtQWF54kB8v3sQ5o9O5\n/UIt4ytyolTu4rmiijr+87nVZKcl8tA1E7VujEgAqNzFU9UNzXz5d3k0t7TyxBdySY3XdnkigaBy\nF880t7Tyn8+tZkdZDY9eN1lvoIoEkK6WEU845/jh4k28v72cn14xgX8bqX1QRQJJI3fxxIL3Cnl+\nxR5uPns4c3QHqkjAqdylx726qpj73tzKJadkcNsFo72OIxKW/Cp3M5tpZvlmVmBmd3by/LfNbLOZ\nrTezd8xsSOCjSjhYtrWU219dz4wRaTxw1an00pUxIt2iy3I3syjgEeAiYBxwjZmNO+K0NUCuc+4U\n4BXg/kAHldC3avchbn5uFWMzUnhs7mTioqO8jiQStvwZuU8FCpxzhc65JuBFYHbHE5xzy5xzde2H\ny4HMwMaUULdxbxU3PP0RA1LjefqLU0nRJY8i3cqfch8MFHU4Lm5/7GhuBN7s7Akzm29meWaWV1ZW\n5n9KCWnbDlQz78kVJMdF8+yN00hPifM6kkjY86fcO5sUdZ2eaDYXyAV+3tnzzrkFzrlc51xuenq6\n/yklZBWW1XDtEyuIierFczdN1+bWIj3En+vci4GsDseZQMmRJ5nZecBdwFnOucbAxJNQtqOshmuf\nWI5zjufnT2do/ySvI4lEDH9G7iuBkWY21MxigTnA4o4nmNlE4HHgMudcaeBjSqjZdqCaqx9fjq/F\n8dxN0xhxUorXkUQiSpfl7pzzAbcAS4EtwCLn3CYzu8fMLms/7edAMvCyma01s8VH+XQSATaXHGbO\nguX0MnjpK9MZM1D7n4r0NL+WH3DOLQGWHPHY3R0+Pi/AuSRErdpdwZeeySMxNornb9JUjIhXdIeq\nBMy7Ww9w3W9X0DcxhkVfOV3FLuIhLRwmAfHKqmLueHU94zJSefqGKfRP1uWOIl5SucsJcc7xy79u\n56F3tjNjRBqPz8slWZtai3hOv4Vy3BqaW7j9lfUsXlfClZMz+f//PoHYaM30iQQDlbscl9LqBm5+\ndjWrdh/i9pmjufms4ZhpETCRYKFyl2O2avchbn52FdUNPn5z3SRmTcjwOpKIHEHlLn5zzvHsij3c\n88YmBvVJ4HdfmsrYDF3DLhKMVO7il8MNzXzvtQ38af0+zhmdzi+vnkjvRK3sKBKsVO7SpbVFlXz9\nhdWUVDZw24Vt8+vaZEMkuKnc5ah8La08+rcdPPTOdgakxrPoK9OZPKSf17FExA8qd+lUQWkNty5a\ny7riKi49dRD3zh6vaRiREKJyl3/R3NLKE+8X8tBft5MYG8Uj107i4lN0NYxIqFG5yydW7znE917b\nwNb91cw8eSD3XH4yJ6XEex1LRI6Dyl0or2nkgbfyeXFlEQNT43niC7mcP26A17FE5ASo3CNYk6+V\nhR/u4qF3tlPf1MKNM4byzfNHaW0YkTCg3+II1NrqeGN9CQ+8tY09FXWcNSqdH1wyjhEnJXsdTUQC\nROUeQZxzLMsv5X+WbmPzvsOMGZjC01+cwtmj07UujEiYUblHAOccb28+wMPvbmfj3sNk9UvgwatP\nZfapg3UzkkiYUrmHsUZfC39cW8KT7+8k/0A1Q9ISuf/KU/j3iYOJidLSvCLhTOUehkqrG3jpoyIW\nLt9NWXUjYwam8IurTuWyUwcRrVIXiQgq9zDR2upYvvMgz6/Yw1827sfX6jhzVDoPXjWMGSPSNKcu\nEmFU7iGuqKKO11bv5ZXVRRRV1JMaH80Xz8jhuulDtEG1SARTuYeg0uoGlqzfx+J1JazeUwnAjBFp\n3Hr+aC48eSAJsVEeJxQRr6ncQ8Tug7W8tekASzftZ9WeQzgHYwamcNuFo7ns1EFk9Uv0OqKIBBGV\ne5BqaG4hb9ch/pZfyrv5pRSW1QIwNiOVb3x+JLMmZDBqQIrHKUUkWKncg0Sjr4UNxVWs2FnBBwXl\n5O0+RJOvldjoXkwflsbcaUM4b+wAstM0QheRrqncPVJa3cDaPZWsKapk9e5DrC2qpNHXCrRNt8yb\nPoQZI9KYPiyNxFh9m0Tk2Kg1uplzjr2V9WzdV83mfYfZsLeKjXur2FfVAEB0L2PcoFTmTh/ClJx+\nTMnpS1pynMepRSTUqdwDxNfSyt7KegrLa9lRWkNBaQ3bS2vYdqCa6gbfJ+cNS09i6tB+TBjcm4nZ\nfTh5UG/iY3R1i4gElsrdT845KmqbKKlsYG9lHcWH6imqqGN3RR17DtZRdKiO5hb3yflpSbGMOCmZ\n2acNYszAVMZmpDJ6YIqW0xWRHhHxTeNraeVQXTMVtU2U1zRSVt1IeU0jpdWNHDjcwP6qBvYfbmBf\nVQNN7XPiH0uOiya7XyKjB6ZwwckDGZaexLD+SQztn6SpFRHxlF/lbmYzgYeAKOC3zrmfHvF8HLAQ\nmAwcBK52zu0KbNTOOedo9LVS0+ijttFHdUPbn5pGH4frmznc0Mzheh+V9U1U1TdTVdfMobomKj/+\nu74Z5z79eeOiezEgNZ4BqXFMGNybC08eyMDUeAb1SSCzb9uf3gkxuq1fRIJSl+VuZlHAI8D5QDGw\n0swWO+c2dzjtRuCQc26Emc0BfgZc3R2BX1q5h8ffK6SusYXaJh91TS20tHbSzkdIjoumd0IMvRNi\n6JsUw6A+CfRNjKVfUixpyW1/90+OIz0ljv7JcaTGR6u4RSRk+TNynwoUOOcKAczsRWA20LHcZwM/\nav/4FeDXZmbOdTYmPjH9kuIYl5FKUmw0iXFRJMZGkRQXTXJcNEmx0aTER5McH01KXAypCdGkxseQ\nEh+t1RBFJKL4U+6DgaIOx8XAtKOd45zzmVkVkAaUdzzJzOYD8wGys7OPK/D54wZo82YRkS74M5zt\nbG7iyBG5P+fgnFvgnMt1zuWmp6f7k09ERI6DP+VeDGR1OM4ESo52jplFA72BikAEFBGRY+dPua8E\nRprZUDOLBeYAi484ZzFwffvHVwLvdsd8u4iI+KfLOff2OfRbgKW0XQr5lHNuk5ndA+Q55xYDTwK/\nN7MC2kbsc7oztIiIfDa/rnN3zi0Blhzx2N0dPm4A/l9go4mIyPHS9YEiImFI5S4iEoZU7iIiYci8\nuqjFzMqA3Z588RPTnyNuzooAkfaaI+31gl5zKBninOvyRiHPyj1UmVmecy7X6xw9KdJec6S9XtBr\nDkealhERCUMqdxGRMKRyP3YLvA7ggUh7zZH2ekGvOexozl1EJAxp5C4iEoZU7ifAzL5jZs7M+nud\npTuZ2c/NbKuZrTezP5hZH68zdRczm2lm+WZWYGZ3ep2nu5lZlpktM7MtZrbJzL7hdaaeYmZRZrbG\nzP7kdZbuoHI/TmaWRdvWg3u8ztID3gbGO+dOAbYB3/U4T7fosKXkRcA44BozG+dtqm7nA251zo0F\npgNfi4DX/LFvAFu8DtFdVO7H70HgdjrZlCTcOOfecs752g+X07amfzj6ZEtJ51wT8PGWkmHLObfP\nObe6/eNq2spusLepup+ZZQIXA7/1Okt3UbkfBzO7DNjrnFvndRYPfAl40+sQ3aSzLSXDvug+ZmY5\nwERghbdJesQvaRuctXodpLv4teRvJDKzvwIDO3nqLuB7wAU9m6h7fdbrdc79sf2cu2j7b/xzPZmt\nB/m1XWQ4MrNk4FXgm865w17n6U5mdglQ6pxbZWZne52nu6jcj8I5d15nj5vZBGAosM7MoG2KYrWZ\nTXXO7e/BiAF1tNf7MTO7HrgE+HwY77Llz5aSYcfMYmgr9uecc695nacHzAAuM7NZQDyQambPOufm\nepwroHSd+wkys11ArnMuFBcg8ouZzQR+AZzlnCvzOk93ad//dxvweWAvbVtMXuuc2+RpsG5kbSOU\n3wEVzrlvep2np7WP3L/jnLvE6yyBpjl38cevgRTgbTNba2aPeR2oO7S/afzxlpJbgEXhXOztZgDz\ngHPbv7dr20e0EuI0chcRCUMauYuIhCGVu4hIGFK5i4iEIZW7iEgYUrmLiIQhlbuISBhSuYuIhCGV\nu4hIGPqZaJm9AAAABUlEQVRfK4sb4RKnxPkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133bc0e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We just adapt the logit function to work our document features $x_i$, and some weights $w_i$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(Y = 0 | X) = \\frac{1}{1 + \\textrm{exp}(w_0 + \\sum_i w_i X_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then finding $P(Y = 1 | X)$ is just $1 - P(Y = 0 | X)$, or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(Y = 1 | X) = \\frac{\\textrm{exp}(w_0 + \\sum_i w_i X_i)}{1 + \\textrm{exp}(w_0 + \\sum_i w_i X_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This second equation, for $P(Y = 1 | X)$, arises directly from the fact that these two terms must sum to 1. Write it out yourself if you need convincing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So how do we train a logistic regression model? Here's where things get a tiny bit trickier than Naive Bayes.\n",
    "\n",
    "In Naive Bayes, the bag-of-words model was 90% of the classifier. Sure, we needed some marginal probabilities and priors, but the word counting was easily the bulk of it.\n",
    "\n",
    "Here, the word counting is still important, but now we have this entire array of *weights* we didn't have before. These weights correspond to feature relevance--how important the features are to prediction. In Naive Bayes we just kind of assumed that was implicit in the count of the words--higher counts, more relevance. But logistic regression separates these concepts, meaning we now have to learn the weights *on our own*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have our training data: $\\{(X^{(j)}, y^{(j)})\\}_{j = 1}^n$, and each $X^{(j)} = (x^{(j)}_1, ..., x^{(j)}_d)$ for $d$ features/dimensions/words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And we want to learn: $\\hat{\\textbf{w}} = \\textrm{argmax}_{\\textbf{w}} \\Pi_{j = 1}^n P(y^{(j)} | X^{(j)}, \\textbf{w})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our conditional log likelihood then takes the form: $l(\\textbf{w}) = \\textrm{ln} \\Pi_j P(y^j | \\vec{x}^j, \\textbf{w})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    " = \\sum_j \\left[ y^j (w_0 + \\sum_i^d w_i x_i^j) - \\textrm{ln}(1 - \\textrm{exp}(w_0 + \\sum_i^d w_i x_i^j)) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![scream](L2/scream.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How did we get here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**First**, note that the likelihood function is typically formally denoted as\n",
    "\n",
    "$$\n",
    "W \\leftarrow \\textrm{arg max}_W \\Pi_l P(Y^l | X^l, W)\n",
    "$$\n",
    "\n",
    "for each training example $X^l$ with corresponding ground-truth label $Y^l$ (they are multipled together because we assume each observation is *independent* of the other). We include the weights $W$ in this expression because the probability is absolutely a function of the weights, and we want to pick the combination of weights $W$ that make the probability expression *as large as possible*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Second**, because we're both pragmatic enough to use a short-cut whenever we can and evil enough to know it'll confuse other people, we never actually work directly with the likelihood as stated above. Instead, we work with the *log*-likelihood, by literally taking the log of the function:\n",
    "\n",
    "$$\n",
    "W \\leftarrow \\textrm{arg max}_W \\sum_l \\textrm{ln} P(Y^l | X^l, W)\n",
    "$$\n",
    "\n",
    "Recall that the log of a product is equivalent to the sum of logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Third**, the probability statement $P(Y^l | X^l, W)$ has two main terms, since $Y$ can be either 1 or 0; we want to pick the one with the largest probability. So we expand that term into the following:\n",
    "\n",
    "$$\n",
    "l(W) = \\sum_l Y^l \\textrm{ln} P(Y^l = 1|X^l, W) + (1 - Y^l) \\textrm{ln} P(Y^l = 0 | X^l, W)\n",
    "$$\n",
    "\n",
    "where $l(W)$ is our log-likehood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this looks somewhat familiar to you: it's a lot like finding the expected value $E[X]$ of a discrete random variable $X$, where you take each possible value $X = x$ and multiply it by its probability $P(X = x)$, summing them all together. You can see the case $Y = 1$ on the left, and $Y = 0$ on the right, both being multiplied by their corresponding conditional probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hopefully you'll *also* note: since you're using this equation for training, $Y^l$ will take ONLY 1 or 0, therefore zero-ing out one side of the equation or the other for every single training instance. So that's kinda nice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Fourth**, get ready for some math! If we have\n",
    "\n",
    "$$\n",
    "l(W) = \\sum_l Y^l \\textrm{ln} P(Y^l = 1|X^l, W) + (1 - Y^l) \\textrm{ln} P(Y^l = 0 | X^l, W)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Expand the last term:\n",
    "\n",
    "$$\n",
    "l(W) = \\sum_l Y^l \\textrm{ln} P(Y^l = 1|X^l, W) + \\textrm{ln} P(Y^l = 0 | X^l, W) - Y^l \\textrm{ln} P(Y^l = 0|X^l, W)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Combine terms with the same $Y^l$ coefficient (first and third terms):\n",
    "\n",
    "$$\n",
    "l(W) = \\sum_l Y^l \\left[ \\textrm{ln} P(Y^l = 1|X^l, W) - \\textrm{ln} P(Y^l = 0|X^l, W) \\right] + \\textrm{ln} P(Y^l = 0 | X^l, W)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall properties of logarithms--when subtracting two logs with the same base, you can combine their arguments into a single log dividing the two:\n",
    "\n",
    "$$\n",
    "l(W) = \\sum_l Y^l \\left[ \\textrm{ln} \\frac{P(Y^l = 1|X^l, W)}{P(Y^l = 0|X^l, W)} \\right] + \\textrm{ln} P(Y^l = 0 | X^l, W)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now things get interesting--remember earlier where we defined exact parametric forms of $P(Y = 1 | X)$ and $P(Y = 0|X)$? Substitute those back in, and you'll get:\n",
    "\n",
    "$$\n",
    "l(W) = \\sum_l \\left[ Y^l (w_0 + \\sum_i^d w_i X_i^l) - \\textrm{ln}(1 - \\textrm{exp}(w_0 + \\sum_i^d w_i X_i^l)) \\right]\n",
    "$$\n",
    "\n",
    "which is exactly the equation we had before we started going through these proofs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Good news!** $l(\\textbf{w})$ is a concave function of $\\textbf{w}$, meaning no pesky local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Bad news!** No closed-form version of $l(\\textbf{w})$ to find explicit values (feel free to try and take its derivative, set it to 0, and solve; it's a transcendental function, so it has no closed-form solution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Good news!** Concave (convex) functions are easy to optimize!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Maximum of a concave function = minimum of a convex function\n",
    "\n",
    " - Gradient ascent (concave) = gradient descent (convex)\n",
    " \n",
    "Gradient: $\\nabla_{\\textbf{w}} l(\\textbf{w}) = \\left[ \\frac{\\partial l(\\textbf{w})}{\\partial w_0}, ..., \\frac{\\partial l(\\textbf{w})}{\\partial w_n} \\right] $\n",
    "\n",
    "Update rule: $w_i^{(t + 1)} = w_i^{(t)} + \\eta \\frac{\\partial l(\\textbf{w})}{\\partial w_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which ultimately leads us to **gradient ascent for logistic regression**.\n",
    "\n",
    "![lr_grad](L2/lr-grad.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**This is Assignment 1!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to going over some basic concepts in probability, Naive Bayes, and Logistic Regression, you'll also implement some document classification code from scratch (don't let me catch anyone using scikit-learn, mmk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The hardest part in the coding will be implementing gradient descent! It's not a lot of code--**especially if you use NumPy vectorized programming**--but it will take some sitting-and-thinking-and-whiteboarding time (unless you know this stuff cold already, I suppose)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also some theory and small proofs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Don't be intimidated.** I purposely made this homework tricky both to get an idea of your level of understanding of the topics so I can gauge how to proceed in the course, and also so you have an idea where your weaknesses are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**ASK ME FOR HELP!** Helping students is *literally* my day job. Don't be shy; if you're stuck, reach out for help, both from me AND your student colleagues!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Administrivia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - **Assignment 1 is out on AutoLab.** This is a warm-up to familiarize you with Python, AutoLab, and to make sure you're up to speed on the basics of machine learning and probability. You'll be implementing Multinomial Naive Bayes and Logistic Regression *from first principles*. Should be fun! Assignment 1 is due *Thursday, August 31 by 11:59pm.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - The first Workshop is *Wednesday, August 30*. If you have not yet signed up for workshops, please do so here: https://docs.google.com/spreadsheets/d/1SmEhL30kBvjPa6WioR34zM8HP5HuY38N3vGzQEwGVS0/edit#gid=0 (Just FYI: after this weekend, I will *auto-assign* anyone who has not yet signed up)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - I will be out of town **all next week and part of the week after.** I agree it's bad timing, but I will be in touch by Slack and email. In the meantime, we'll have guest lecturers discussing their work in different areas of data science, so please be sure to attend if you're at all interested in seeing what different forms data science can take!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
